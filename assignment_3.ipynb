{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fcc77e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import normalized_mutual_info_score \n",
    "from scipy.spatial import distance, distance_matrix\n",
    "from scipy.spatial.distance import cdist\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c64aada-ca6d-4fac-9c50-ff27f17c909c",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f75e582f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# KDD Dataset\n",
    "# First column is the BLOCK ID (class label), BLOCK IDs are integers running from 1 to 303 with 153 unique values (k)\n",
    "# Second column is the ELEMENT ID (sample number), unique numbers, not ordered\n",
    "# Third column is the class of the example. Homologous proteins = 1, non-homologous proteins = 0\n",
    "data = pd.read_csv('bio_train.csv',skiprows=0).to_numpy(dtype='object')\n",
    "\n",
    "#####################\n",
    "# Toy Dataset\n",
    "#dataset = np.genfromtxt('dataset1_noCluster7.csv', delimiter = ',')[1:]\n",
    "#dataset_ft = dataset[:,:2]\n",
    "#dataset_lb = dataset[:,-1]\n",
    "\n",
    "#scaler = StandardScaler()\n",
    "#dataset_ft = scaler.fit_transform(dataset_ft)\n",
    "\n",
    "#plt.scatter(dataset_ft[:,0], dataset_ft[:,1], c = dataset_lb)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3dee64-6fe3-4525-8975-94db19e8025a",
   "metadata": {},
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3734f026",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle, split into labels/features and normalize data\n",
    "def process_data(data):\n",
    "    # Shuffle\n",
    "    shuffle = np.random.permutation(len(data))\n",
    "    data = data[shuffle]\n",
    "    \n",
    "    # Split\n",
    "    block_ids = data[:,0]\n",
    "    element_ids = data[:,1]\n",
    "    homology = data[:,2]\n",
    "    features = data[:,3:]\n",
    "    \n",
    "    # Normalize the features\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    return block_ids, element_ids, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be3c79a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor j in range(len(features)):\\n    for i in range(len(features[0])):\\n        if not isinstance(features[j][i], float):\\n            print(features[j][i], type(features[j][i]))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess the data\n",
    "block_ids, element_ids, features = process_data(data)\n",
    "features = features.astype(np.float64) # only important if no normalization is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb4af20-7dd3-405a-89f7-ecfd3c799188",
   "metadata": {},
   "source": [
    "### Task 1 - Lloyds Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee23b261",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lloyds(data, k=153):\n",
    "    # Number of samples and features of the dataset\n",
    "    \n",
    "    n_samples, n_features = np.shape(data)\n",
    "    \n",
    "    # Pick k random points from data to be the initial cluster centers (eventually use kmeans+ here?)\n",
    "    #rand_nums = np.random.randint(0,n_samples,k)\n",
    "    #cluster_means = data[rand_nums]\n",
    "    \n",
    "    # pick the first k points as initial cluster means\n",
    "    cluster_means = data[:k]\n",
    "    \n",
    "    old_means = np.zeros([k, n_features])\n",
    "    counter = 0\n",
    "    \n",
    "    while (old_means != cluster_means).any():\n",
    "    \n",
    "        counter += 1\n",
    "        old_means = np.copy(cluster_means)\n",
    "        \n",
    "        # avoiding endless loop\n",
    "        if counter == 1000:\n",
    "            break\n",
    "            \n",
    "        # printing progress\n",
    "        if counter % 20 == 0: \n",
    "            print(\"iteration: \",counter)\n",
    "\n",
    "            \n",
    "        # measure assingment runtime\n",
    "        #start_assign = time.perf_counter()\n",
    "        \n",
    "        ############# Assign step\n",
    "        \n",
    "        distance_matrix = cdist(data,cluster_means, metric='sqeuclidean')\n",
    "        cluster_labels = np.argmin(distance_matrix, axis=1)\n",
    "        \n",
    "        ############# Update step\n",
    "        for j in range(k):\n",
    "            \n",
    "            idcs = np.where(cluster_labels == j)[0]\n",
    "            cluster_size = len(idcs)\n",
    "            \n",
    "            if cluster_size > 0:\n",
    "                cluster_sum = np.sum(data[idcs], axis=0)\n",
    "                cluster_means[j] = cluster_sum/cluster_size\n",
    "            \n",
    "            \n",
    "        #end_update = time.perf_counter()\n",
    "        \n",
    "        #print('Assign step runtime: '+str(start_update - start_assign))\n",
    "        #print('Update step runtime: '+str(end_update - start_update))\n",
    "        \n",
    "    print('KMeans converged in '+str(counter)+' iterations.')\n",
    "    return cluster_labels, cluster_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b6ccb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration:  20\n",
      "iteration:  40\n",
      "iteration:  60\n",
      "iteration:  80\n",
      "iteration:  100\n",
      "iteration:  120\n",
      "iteration:  140\n",
      "iteration:  160\n",
      "iteration:  180\n",
      "iteration:  200\n",
      "iteration:  220\n",
      "iteration:  240\n",
      "iteration:  260\n",
      "iteration:  280\n",
      "iteration:  300\n",
      "iteration:  320\n",
      "iteration:  340\n",
      "iteration:  360\n",
      "iteration:  380\n",
      "iteration:  400\n",
      "iteration:  420\n",
      "KMeans converged in 430 iterations.\n"
     ]
    }
   ],
   "source": [
    "labels, centers = lloyds(features)\n",
    "#labels, centers = lloyds(dataset_ft, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1b5ffd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(dataset_ft[:,0], dataset_ft[:,1], c = labels)\n",
    "#plt.scatter(centers[:,0], centers[:,1], c='r')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e6240eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18974373867283634\n"
     ]
    }
   ],
   "source": [
    "NMI_score = normalized_mutual_info_score(block_ids, labels)\n",
    "print(NMI_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f202e35d-2956-4ef8-9832-307a0ef484cb",
   "metadata": {},
   "source": [
    "### Task 2 - LSH + Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "361dddb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function defines a hash function according to the notes on LSH + Kmeans and assigns \n",
    "# the samples to the buckets.\n",
    "# There is still a mistake in this function\n",
    "\n",
    "def hash_simple(data, no_buckets):\n",
    "    \n",
    "    no_samples = len(data)\n",
    "    \n",
    "    hash_values = np.zeros(no_samples)\n",
    "    \n",
    "    vector_p = np.random.normal(loc=0.0, scale=1.0, size=len(data[0]))\n",
    "    \n",
    "    for i in range(n):\n",
    "        hash_values[i] = data[i].dot(vector_p)\n",
    "        \n",
    "    min_val = np.min(hash_values)\n",
    "    max_val = np.max(hash_values)\n",
    "    \n",
    "    bucket_size = (max_val-min_val) / (no_buckets-1)\n",
    "    \n",
    "    return np.floor(hash_values/bucket_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8de36888-1f1d-4f22-ae65-fa3e6694cf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This class defines a given number of hash functions. \n",
    "# The calculate_hash_values function can be used to calclueate the hash values of any array with no_features as the second dimension\n",
    "class hash_functions:\n",
    "    def __init__(self, no_functions, w, no_features):\n",
    "        self.w = w #scalar\n",
    "        self.b = np.zeros(no_functions) # vector\n",
    "        self.a = np.random.normal(loc=0.0, scale=1.0, size=(no_functions, no_features)) # matrix\n",
    "\n",
    "    def calculate_hash_values(self,data):\n",
    "        hash_values = (np.dot(data, self.a.T) + self.b) / self.w\n",
    "        return hash_values\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "021e5e1b-dd4f-4055-b96c-a4e59902883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define hash functions\n",
    "no_features = len(features[0])\n",
    "w=3\n",
    "hash_funcs = hash_functions(16, w, no_features)\n",
    "\n",
    "#calculate the 16 hash values from all features\n",
    "hash_values = hash_funcs.calculate_hash_values(features)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:datascience1] *",
   "language": "python",
   "name": "conda-env-datascience1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
